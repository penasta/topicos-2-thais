---
title: "XGBoost/Catboost"
subtitle: "Grupo 3"
author:
  - name: Bruno Gondim Toledo
    url: https://github.com/penasta
  - name: João Victor
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: auto
    scrollable: true
    logo: unb.jpg
    theme: serif
    width: 1500
    css: styles.css
    footer: Departamento de estatística - UnB
resources:
  - demo.pdf
editor_options: 
  chunk_output_type: console
knitr:
  opts_chunk:
    dev: png
    dev.args:
      bg: transparent
---

## Introdução

XGBoost (eXtreme Gradient Boosting) (Tianqi Chen, 2016) é uma das implementações de three ensembling baseada em gradient boosting mais populares, por ser otimizado para desempenho e eficiência.

A ideia do algoritmo gradient boosting é realizar ensemble de M classificadores fracos para obter um modelo robusto que faça boas previsões. Estes classificadores serão árvores de decisão. Porém, diferentemente do AdaBoost estudado anteriormente, a regularização é feita a partir da minimização de uma função de perda diferenciável, e não pela atribuição de maior peso a instâncias classificadas erroneamente.

## Introdução

Por se tratar de um modelo sequencial, em que a árvore $i+2$ necessitará da informação dos resíduos da árvore $i+1$, também herda a restrição de paralelismo que outros modelos de three ensembling também contém. Porém, a implementação do algoritmo XGBoost busca paralelizar todas as rotinas necessárias para construção do modelo, tornando sequencial apenas o início da construção de cada árvore.

```{css, echo = FALSE}
.center {
  text-align: center !important
}
```

:::{.center}
```{mermaid}
graph LR
    A[Dados] --> B[Modelo 1];
    B --> C[Modelo 2];
    C --> D[...];
    D --> E[Modelo m];
    E --> F[Ensembling dos modelos];
    F --> G[Modelo final];
```
:::

## Gradient Boosting

O algoritmo base do XGBoost é o Gradient Boosting, no qual busca-se minimizar uma função de perda utilizando gradiente descendente. Esta otimização **não** será numérica, mas sim fornecida pela combinação das árvores geradas (boosting), que irão conduzir o gradiente para o mínimo global da função.

Cada um desses aprendizes fracos sequenciais irão focar nos resíduos da árvore anterior, buscando a divisão que minimize a função de perda definida (podendo ser *default* ou escolhida pelo usuário).

## Gradient Boosting

Seja M o número de árvores fixadas para construção do modelo, o modelo final será dado pela fórmula $$F_M(x) = F_0(x) +\sum^M_{m=1}F_m(x)$$ Em que $F_M(x)$ será o modelo final para predições, $F_0(x)$ será um modelo inicial de árvore, $F_1(x)$ será o modelo que minimiza os resíduos do modelo $F_0(x)$; $F_2(x)$ será o modelo que minimiza os resíduos do modelo $F_1(x)$, e assim por diante até o modelo $F_{m}(x)$. Portanto, a classificação da i-ésima instância será dada por:$$\hat{y_i} = F_M(x_i)$$

## Construção das árvores

Salvo a primeira árvore, que será um chute inicial, iremos definir uma função objetivo com sendo$$obj = \sum^n_{i=1}l(y_i,\hat{y}_i^{(t)})+\sum^t_{i=1}\Omega(f_i),$$ em que o termo $l(y_i,\hat{y}_i^{(t)})$ irá computar a perda relativa no t-ésimo passo, e $\Omega(f_i)$ será o t-ésimo termo de regularização, e $obj$ será baseado nos resíduos da árvore anterior.

O algoritmo escolhe a cada passo a árvore que minimiza $obj$ com base nas *features* selecionadas automaticamente pelo modelo.

## Under the hood

Para a seleção de *features* que irão minimizar a função objetivo, o algoritmo irá avaliar diversas possíveis divisões em diferentes *features*, construindo entre outras coisas histogramas para as *features* que ajudem a encontrar o melhor ponto de divisão. Para isso, o algoritmo utiliza do paralelismo, executando diversas divisões e encontrando a melhor possível de forma iterativa e rápida.

Desta forma, apesar de se tratar de um modelo sequencial, o algoritmo implementado utiliza em todas as etapas que for possível da computação paralelizada, afim de aproveitar ao máximo o desempenho computacional e retornar um modelo que seja robusto e **rápido**

Para maior aprofundamento teórico sobre o funcionamento do modelo, consulte [XGBoost tutorial](https://xgboost.readthedocs.io/_/downloads/en/release_0.80/pdf/), seção 1.3

## Flow do modelo

Uma forma simplificada de mostrar o funcionamento do XGBoost pode ser representada pelo seguinte grafo:

:::{.center}
```{mermaid}
graph TD

    A[Entrada de dados] --> B
    B[Preparação dos Dados] --> B1 & B2 & B3
    B1[Limpeza dos Dados]
    B2[Normalização]
    B3[Divisão Treino/Teste]
    B1 & B2 & B3 --> B_Final[Dados Preparados]

    B_Final --> C[Entrada de Dados no Modelo]

    C --> D[Construção da Primeira Árvore]
    D --> D1 & D2 & D3
    D1[Avaliação das Divisões]
    D2[Construção dos Nós]
    D3[Combinação das Previsões]
    D1 & D2 & D3 --> D_Final[Árvore Construída]
    
    D_Final --> E[Entrada de Dados Residual para a Segunda Árvore]

    E --> F[Construção da Segunda Árvore]
    F --> F1 & F2 & F3
    F1[Avaliação das Divisões]
    F2[Construção dos Nós]
    F3[Combinação das Previsões]
    F1 & F2 & F3 --> F_Final[Segunda Árvore Construída]

    F_Final --> G[Repetir o processo anterior m vezes]

    G --> H[Avaliação do Modelo]
    
    H --> I[Ensembling das Árvores com Pesos Relativos]

    I --> J[Modelo final]

```
:::

## Implementação XGBoost

(vou adicionar ainda)

## Exemplo prático: XGBoost, CatBoost & outros

(vou organizar esta seção ainda, trazendo um exemplo prático que fiz uma vez comparando algoritmos)

https://github.com/penasta/NLP/blob/main/funcao_nlp.ipynb

## Referências

(vou ajustar isso ainda, depois)

https://bdta.abcd.usp.br/directbitstream/6962846b-66bd-4bd6-9f74-2b18bff03234/Pinheiro_JoãoManoelHerrera_tcc.pdf

https://xgboost.readthedocs.io/_/downloads/en/release_0.80/pdf/
